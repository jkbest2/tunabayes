
@article{Carpenter2017,
  title = {Stan: {{A}} Probabilistic Programming Language},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Softw.},
  date = {2017},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  file = {/home/jkbest/Documents/Literature/Carpenter et al - 2017 - Stan.pdf}
}

@article{Maunder2003,
  title = {Is It Time to Discard the {{Schaefer}} Model from the Stock Assessment Scientist's Toolbox?},
  volume = {61},
  issn = {01657836},
  doi = {10.1016/S0165-7836(02)00273-4},
  number = {1-3},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  date = {2003},
  pages = {145-154},
  author = {Maunder, Mark N. and Prager, Michael H.},
  file = {/home/jkbest/Zotero/storage/9FICDJBI/Maunder - 2003 - Is it time to discard the Schaefer model from the .pdf},
  isbn = {0165-7836}
}

@article{Cressie2009,
  title = {Accounting for Uncertainty in Ecological Analysis: The Strengths and Limitations of Hierarchical Statistical Modeling},
  volume = {19},
  issn = {1051-0761},
  url = {http://doi.wiley.com/10.1890/07-0744.1},
  doi = {10.1890/07-0744.1},
  number = {3},
  journaltitle = {Ecological Applications},
  shortjournal = {Ecol. Appl.},
  urldate = {2018-04-16},
  date = {2009-04-01},
  pages = {553-570},
  keywords = {Bayesian modeling,data model,design,empirical Bayes,harbor seals,MCMC,prior,process model,spatial process,spatiotemporal process},
  author = {Cressie, Noel A. C. and Calder, Catherine A. and Clark, James S. and Ver Hoef, Jay M. and Wikle, Christopher K.},
  file = {/home/jkbest/Zotero/storage/IPK4XKIY/Cressie et al. - 2009 - Accounting for uncertainty in ecological analysis.pdf;/home/jkbest/Zotero/storage/QNGRHD9E/Cressie et al. - 2009 - Accounting for uncertainty in ecological analysis.pdf}
}

@article{Papaspiliopoulos2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0708.3797},
  title = {A General Framework for the Parametrization of Hierarchical Models},
  volume = {22},
  issn = {0883-4237},
  url = {http://projecteuclid.org/euclid.ss/1185975637},
  doi = {10.1214/088342307000000014},
  abstract = {In this paper, we describe centering and noncentering methodology as complementary techniques for use in parametrization of broad classes of hierarchical models, with a view to the construction of effective MCMC algorithms for exploring posterior distributions from these models. We give a clear qualitative understanding as to when centering and noncentering work well, and introduce theory concerning the convergence time complexity of Gibbs samplers using centered and noncentered parametrizations. We give general recipes for the construction of noncentered parametrizations, including an auxiliary variable technique called the state-space expansion technique. We also describe partially noncentered methods, and demonstrate their use in constructing robust Gibbs sampler algorithms whose convergence properties are not overly sensitive to the data.},
  number = {1},
  journaltitle = {Statistical Science},
  shortjournal = {Stat. Sci.},
  date = {2007},
  pages = {59-73},
  keywords = {MCMC,hierarchical models,chastic processes,latent sto,parametrization},
  author = {Papaspiliopoulos, Omiros and Roberts, Gareth O. and Sköld, Martin},
  file = {/home/jkbest/Zotero/storage/9LABV9M2/Papaspiliopoulos, Roberts, Sköld - 2007 - A General Framework for the Parametrization of Hierarchical Models.pdf;/home/jkbest/Zotero/storage/IGEHAE88/Papaspiliopoulos et al. - 2007 - A General Framework for the Parametrization of Hie.pdf}
}

@incollection{Betancourt2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.0906},
  location = {{Boca Raton, FL}},
  title = {Hamiltonian {{Monte Carlo}} for Hierarchical Models},
  isbn = {9761482235111},
  url = {http://arxiv.org/abs/1312.0906},
  abstract = {Hierarchical modeling provides a framework for modeling the complex interactions typical of problems in applied statistics. By capturing these relationships, however, hierarchical models also introduce distinctive pathologies that quickly limit the efficiency of most common methods of in- ference. In this paper we explore the use of Hamiltonian Monte Carlo for hierarchical models and demonstrate how the algorithm can overcome those pathologies in practical applications.},
  booktitle = {Current {{Trends}} in {{Bayesian Methodology}} with {{Applications}}},
  publisher = {{CRC press}},
  date = {2015},
  pages = {79-101},
  author = {Betancourt, M. J. and Girolami, Mark},
  editor = {Upadhyay, Satyanshu Kumar and Singh, Umesh and Dey, Dipak K. and Loganathan, Appaia},
  file = {/home/jkbest/Zotero/storage/AGA3HHDT/Betancourt and Girolami - 2015 - Hamiltonian Monte Carlo for Hierarchical Models.pdf},
  doi = {10.1201/b18502-5}
}

@article{Aeberhard2018,
  title = {Review of State-Space Models for Fisheries Science},
  volume = {5},
  issn = {2326-8298},
  url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100427},
  doi = {10.1146/annurev-statistics-031017-100427},
  abstract = {Fisheries science is concerned with the management and understanding of the raising and harvesting of fish. Fish stocks are assessed using biological and fisheries data with the goal of estimating either their total population or biomass. Stock assessment models also make it possible to predict how stocks will respond to varying levels of fishing pressure in the future. Such tools are essential with overfishing now reducing stocks and employment worldwide, with in turn many serious social, economic, and environmental implications. Increasingly, a state-space framework is being used in place of deterministic and standard parametric stock assessment models. These efforts have not only had considerable impact on fisheries management but have also advanced the supporting statistical theory and inference tools as well as the required software. An application of such techniques to the North Sea cod stock highlights what should be considered best practices for science-based fisheries management.},
  number = {1},
  journaltitle = {Annual Review of Statistics and Its Application},
  shortjournal = {Annu. Rev. Stat. Appl.},
  urldate = {2018-05-09},
  date = {2018-03-07},
  pages = {215-235},
  keywords = {population dynamics,state space models,fisheries,stock assessment,fish stock assessment,random effects prediction,state-space assessment model},
  author = {Aeberhard, William H. and Mills Flemming, Joanna and Nielsen, Anders},
  file = {/home/jkbest/Zotero/storage/K7XTFKIA/Aeberhard et al. - 2018 - Review of State-Space Models for Fisheries Science.pdf}
}

@article{Winker2018,
  title = {{{JABBA}}: {{Just Another Bayesian Biomass Assessment}}},
  volume = {204},
  issn = {01657836},
  url = {https://www.sciencedirect.com/science/article/pii/S0165783618300845},
  doi = {10.1016/j.fishres.2018.03.010},
  abstract = {This study presents a new, open-source modelling software entitled ‘Just Another Bayesian Biomass Assessment’ (JABBA). JABBA can be used for biomass dynamic stock assessment applications, and has emerged from the development of a Bayesian State-Space Surplus Production Model framework, already applied in stock assessments of sharks, tuna, and billfishes around the world. JABBA presents a unifying, flexible framework for biomass dynamic modelling, runs quickly, and generates reproducible stock status estimates and diagnostic tools. Specific emphasis has been placed on flexibility for specifying alternative scenarios, achieving high stability and improved convergence rates. Default JABBA features include: 1) an integrated state-space tool for averaging and automatically fitting multiple catch per unit effort (CPUE) time series; 2) data-weighting through estimation of additional observation variance for individual or grouped CPUE; 3) selection of Fox, Schaefer, or Pella-Tomlinson production functions; 4) options to fix or estimate process and observation variance components; 5) model diagnostic tools; 6) future projections for alternative catch regimes; and 7) a suite of inbuilt graphics illustrating model fit diagnostics and stock status results. As a case study, JABBA is applied to the 2017 assessment input data for South Atlantic swordfish (Xiphias gladius). We envision that JABBA will become a widely used, open-source stock assessment tool, readily improved and modified by the global scientific community.},
  issue = {November 2017},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2018-05-09},
  date = {2018-08-01},
  pages = {275-288},
  keywords = {Bayesian,JAGS,State-space framework,Stock assessment,Surplus production model},
  author = {Winker, Henning and Carvalho, Felipe and Kapur, Maia},
  file = {/home/jkbest/Zotero/storage/4AL26ZEI/Winker et al. - 2018 - JABBA Just Another Bayesian Biomass Assessment.pdf}
}

@article{Meyer1999,
  title = {{{BUGS}} in {{Bayesian}} Stock Assessments},
  volume = {56},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/10.1139/f99-043},
  doi = {10.1139/f99-043},
  abstract = {This paper illustrates the ease with which Bayesian nonlinear state–space models can now be used for practical fisheries stock assessment. Sampling from the joint posterior density is accomplished using Gibbs sampling via BUGS, a freely available software package. By taking advantage of the model representation as a directed acyclic graph, BUGS automates the hitherto tedious calculation of the full conditional posterior distributions. Moreover, the output from BUGS can be read directly into the software CODA for convergence diagnostics and statistical summary. We illustrate the BUGS implementation of a nonlinear nonnormal state–space model using a Schaefer surplus production model as a basic example. This approach extends to other assessment methodologies, including delay difference and age-structured models.},
  number = {6},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-08-10},
  date = {1999-06},
  pages = {1078-1087},
  author = {Meyer, Renate and Millar, Russell B.},
  file = {/home/jkbest/Zotero/storage/KJZ33GEI/Meyer, Millar - 1999 - BUGS in Bayesian stock assessments.pdf;/home/jkbest/Zotero/storage/ZKTKFPAT/NotesMeyerMillar1999.pdf},
  isbn = {0706-652X},
  eprinttype = {pmid},
  eprint = {4549946}
}

@article{Millar2000,
  title = {Non-Linear State Space Modelling of Fisheries Biomass Dynamics by Using {{Metropolis}}-{{Hastings}} within-{{Gibbs}} Sampling},
  volume = {49},
  issn = {0035-9254},
  url = {http://doi.wiley.com/10.1111/1467-9876.00195},
  doi = {10.1111/1467-9876.00195},
  abstract = {State space modelling and Bayesian analysis are both active areas of applied research in ®sheries stock assessment. Combining these two methodologies facilitates the ®tting of state space models that may be non-linear and have non-normal errors, and hence it is particularly useful for modelling ®sheries dynamics. Here, this approach is demonstrated by ®tting a non-linear surplus production model to data on South Atlantic albacore tuna (Thunnus alalunga). The state space approach allows for random variability in both the data (the measurement of relative biomass) and in annual biomass dynamics of the tuna stock. Sampling from the joint posterior distribution of the unobservables was achieved by using Metropolis±Hastings within-Gibbs sampling.},
  number = {3},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  shortjournal = {J. Royal Stat. Soc. B},
  urldate = {2018-08-10},
  date = {2000-08-01},
  pages = {327-342},
  keywords = {Markov chain Monte Carlo sampling,Bayesian analysis,Fish stock assessment,Non‐linear state space models,Surplus production models,Tuna},
  author = {Millar, Russell B. and Meyer, Renate},
  file = {/home/jkbest/Zotero/storage/ZMW58IR9/Millar and Meyer - 2000 - Non-linear state space modelling of fisheries biom.pdf}
}

@article{Schaefer1954,
  title = {Some Aspect of the Dynamics of Populations Important to the Management of the Commercial Marine Fisheries},
  volume = {1},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  number = {2},
  journaltitle = {Inter-American Tropical Tuna Commission Bulletin},
  shortjournal = {Inter-Am. Trop. Tuna Comm. Bull.},
  date = {1954},
  pages = {27-56},
  keywords = {Fish Population,Fishing Effort,Natural Rate,Singular Point,Stable Equilibrium},
  author = {Schaefer, Milner B},
  file = {/home/jkbest/Documents/Literature/Schaefer - 1991 - Some aspects of the dynamics of populations important to the management of the.pdf;/home/jkbest/Zotero/storage/L3KPL3FC/Schaefer - 1954 - Some aspect of the dynamics of populations importa.pdf},
  isbn = {9788578110796},
  eprinttype = {pmid},
  eprint = {25246403}
}

@article{Monnahan2018,
  title = {No-{{U}}-Turn Sampling for Fast {{Bayesian}} Inference in {{ADMB}} and {{TMB}}: {{Introducing}} the Adnuts and Tmbstan {{R}} Packages},
  volume = {13},
  issn = {1932-6203},
  url = {http://dx.plos.org/10.1371/journal.pone.0197954},
  doi = {10.1371/journal.pone.0197954},
  abstract = {Statistical inference is a widely-used, powerful tool for learning about natural processes in diverse fields. The statistical software platforms AD Model Builder (ADMB) and Template Model Builder (TMB) are particularly popular in the ecological literature, where they are typically used to perform frequentist inference of complex models. However, both lack capabilities for flexible and efficient Markov chain Monte Carlo (MCMC) integration. Recently, the no-U-turn sampler (NUTS) MCMC algorithm has gained popularity for Bayesian inference through the software Stan because it is efficient for high dimensional, complex hierarchical models. Here, we introduce the R packages adnuts and tmbstan, which provide NUTS sampling in parallel and interactive diagnostics with ShinyStan. The ADMB source code was modified to provide NUTS, while TMB models are linked directly into Stan. We describe the packages, provide case studies demonstrating their use, and contrast performance against Stan. For TMB models, we show how to test the accuracy of the Laplace approximation using NUTS. For complex models, the performance of ADMB and TMB was typically within +/- 50\% the speed of Stan. In one TMB case study we found inaccuracies in the Laplace approximation, potentially leading to biased inference. adnuts provides a new method for estimating hierarchical ADMB models which previously were infeasible. TMB users can fit the same model in both frequentist and Bayesian paradigms, including using NUTS to test the validity of the Laplace approximation of the marginal likelihood for arbitrary subsets of parameters. These software developments extend the available statistical methods of the ADMB and TMB user base with no additional effort by the user.},
  number = {5},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2018-05-31},
  date = {2018-05-24},
  pages = {e0197954},
  author = {Monnahan, Cole C. and Kristensen, Kasper},
  editor = {Deng, Yong},
  file = {/home/jkbest/Zotero/storage/IX7ZM66S/Monnahan and Kristensen - 2018 - No-U-turn sampling for fast Bayesian inference in .pdf}
}

@article{Bolker2013,
  title = {Strategies for Fitting Nonlinear Ecological Models in {{R}}, {{AD Model Builder}}, and {{BUGS}}},
  volume = {4},
  issn = {2041210X},
  url = {http://doi.wiley.com/10.1111/2041-210X.12044},
  doi = {10.1111/2041-210X.12044},
  number = {6},
  journaltitle = {Methods in Ecology and Evolution},
  shortjournal = {Methods Ecol. Evol.},
  urldate = {2018-05-31},
  date = {2013-06},
  pages = {501-512},
  keywords = {R,JAGS,AD Model Builder,optimization,parameter estimation,WinBUGS},
  author = {Bolker, Benjamin M. and Gardner, Beth and Maunder, Mark and Berg, Casper W. and Brooks, Mollie and Comita, Liza and Crone, Elizabeth and Cubaynes, Sarah and Davies, Trevor and de Valpine, Perry and Ford, Jessica and Gimenez, Olivier and Kéry, Marc and Kim, Eun Jung and Lennert-Cody, Cleridy and Magnusson, Arni and Martell, Steve and Nash, John and Nielsen, Anders and Regetz, Jim and Skaug, Hans and Zipkin, Elise F.},
  editor = {Ramula, Satu},
  options = {useprefix=true},
  file = {/home/jkbest/Zotero/storage/P5FYQBBX/Bolker et al. - 2013 - Strategies for fitting nonlinear ecological models.pdf}
}

@article{Monnahan2017,
  title = {Faster Estimation of {{Bayesian}} Models in Ecology Using {{Hamiltonian Monte Carlo}}},
  volume = {8},
  issn = {2041210X},
  url = {http://doi.wiley.com/10.1111/2041-210X.12681},
  doi = {10.1111/2041-210X.12681},
  abstract = {Despite the potential of HMC, and the availability of Stan, adoption has been slow in ecology, likely because ecologists are either unaware of its existence, or are unsure when it should be preferred over BUGS. Here, we illustrate the principles that underlie HMC and then compare the efficiency between Stan and a BUGS variant, JAGS (Plummer 2003), across a range of models in population ecology. Specifically, we test how HMC performance scales with model size and complexity, and its suitability for hierarchical models. Our goal is to explore the relative benefits of Stan and JAGS and to provide guidance for ecologists looking to use the power of HMC for faster and more robust Bayesian inference. Stan is faster than JAGS.},
  number = {3},
  journaltitle = {Methods in Ecology and Evolution},
  shortjournal = {Methods Ecol. Evol.},
  date = {2017-03},
  pages = {339-348},
  author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
  editor = {O'Hara, Robert B.},
  file = {/home/jkbest/Zotero/storage/XW7GTNWY/Monnahan, Thorson, Branch - 2017 - Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo.pdf;/home/jkbest/Zotero/storage/YPL4DYSW/Monnahan et al. - 2017 - Faster estimation of Bayesian models in ecology us.pdf},
  isbn = {2041-210x}
}

@article{Kleppe2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.02068},
  primaryClass = {stat},
  langid = {english},
  title = {Dynamically Rescaled {{Hamiltonian Monte Carlo}} for {{Bayesian Hierarchical Models}}},
  url = {http://arxiv.org/abs/1806.02068},
  abstract = {Dynamically rescaled Hamiltonian Monte Carlo (DRHMC) is introduced as a computationally fast and easily implemented method for performing full Bayesian analysis in hierarchical statistical models. The method relies on introducing a modiﬁed parameterisation so that the re-parameterised target distribution has close to constant scaling properties, and thus is easily sampled using standard (Euclidian metric) Hamiltonian Monte Carlo. Provided that the parameterisations of the conditional distributions specifying the hierarchical model are “constant information parameterisations” (CIP), the relation between the modiﬁed- and original parameterisation is bijective, explicitly computed and admit exploitation of sparsity in the numerical linear algebra involved. CIPs for a large catalogue of statistical models are presented, and from the catalogue, it is clear that many CIPs are currently routinely used in statistical computing. A relation between the proposed methodology and a class of explicitly integrated Riemann manifold Hamiltonian Monte Carlo methods is discussed. The methodology is illustrated on several example models, including a model for inﬂation rates with multiple levels of non-linearly dependent latent variables.},
  urldate = {2018-09-11},
  date = {2018-06-06},
  keywords = {Statistics - Methodology,Statistics - Computation},
  author = {Kleppe, Tore Selland},
  file = {/home/jkbest/Documents/Literature/Kleppe - 2018 - Dynamically rescaled Hamiltonian Monte Carlo for Bayesian Hierarchical Models2.pdf;/home/jkbest/Zotero/storage/IBKL2PMA/Kleppe - 2018 - Dynamically rescaled Hamiltonian Monte Carlo for B.pdf}
}

@article{Walters1994,
  title = {Calculation of {{Bayes}} Posterior Probability Distributions for Key Population Parameters},
  volume = {51},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f94-071},
  doi = {10.1139/f94-071},
  abstract = {The Bayes posterior probability distribution is a powerful way to represent uncertainty in fisheries stock assessments, and can be calculated for key population and policy parameters of practically any population dynamics model. But the calculation is unwieldy when probabilities are to be assigned to a large grid of parameter combinations. The computational burden can be reduced substantially by analytically integrating over at least two "nuisance parameters" that occur in most assessment models: the observation error variance and the catchability coefficient. This simplification allows the analyst and manager to focus more easily on population parameters (stock size, slope of recruitment curve) that are of direct policy interest., La distribution bayésienne de probabilité a posteriori constitue un outil puissant pour représenter les incertitudes dans les évaluations des stocks halieutiques, et peut être calculée pour des paramètres clés de la population et des politiques dans pratiquement n'importe quel modèle de la dynamique des populations. Toutefois, ce calcul est trop compliqué lorsqu'il faut attribuer des probabilités à une large grille de combinaisons de paramètres. Le fardeau du calcul peut être fortement réduit si on fait une intégration analytique d'au moins deux paramètres dérangeants qui sont présents dans la plupart des modèles d'évaluation : la variance de l'erreur d'observation et le coefficient de capturabilité. Cette simplification permet à l'analyste et au gestionnaire de s'intéresser plus directement aux paramètres sur les populations (taille du stock, pente de la courbe du recrutement) qui présentent un intérêt direct pour les politiques.},
  number = {3},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-09-18},
  date = {1994-03-01},
  pages = {713-722},
  author = {Walters, Carl and Ludwig, Donald},
  file = {/home/jkbest/Documents/Literature/Walters Ludwig - 1994 - Calculation of Bayes Posterior Probability Distributions for Key Population.pdf;/home/jkbest/Zotero/storage/DB8Q4VVT/f94-071.html}
}

@article{Schnute1994,
  langid = {english},
  title = {A General Framework for Developing Sequential Fisheries Models},
  volume = {51},
  issn = {0706-652X, 1205-7533},
  url = {http://www.nrcresearchpress.com/doi/10.1139/f94-168},
  doi = {10.1139/f94-168},
  number = {8},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-10-08},
  date = {1994-08},
  pages = {1676-1688},
  author = {Schnute, Jon T.},
  file = {/home/jkbest/Zotero/storage/XP3IBCVZ/Schnute - 1994 - A General Framework for Developing Sequential Fish.pdf}
}

@article{Betancourt2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.02434},
  primaryClass = {stat},
  langid = {english},
  title = {A Conceptual Introduction to {{Hamiltonian Monte Carlo}}},
  url = {http://arxiv.org/abs/1701.02434},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on diﬃcult problems and how it is best applied in practice. Unfortunately, that understanding is conﬁned within the mathematics of diﬀerential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
  urldate = {2018-11-03},
  date = {2017-01-09},
  keywords = {Statistics - Methodology},
  author = {Betancourt, Michael},
  file = {/home/jkbest/Zotero/storage/D8TJFCE9/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf}
}

@article{Lunn2000,
  langid = {english},
  title = {{{WinBUGS}} - a {{Bayesian}} Modelling Framework: Concepts, Structure, and Extensibility},
  volume = {10},
  issn = {1573-1375},
  url = {https://doi.org/10.1023/A:1008929526011},
  doi = {10.1023/A:1008929526011},
  shorttitle = {{{WinBUGS}} - {{A Bayesian}} Modelling Framework},
  abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.},
  number = {4},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat. Comput.},
  urldate = {2018-11-07},
  date = {2000-10-01},
  pages = {325-337},
  keywords = {WinBUGS,BUGS,directed acyclic graphs,Markov chain Monte Carlo,object-orientation,run-time linking,type extension},
  author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  file = {/home/jkbest/Documents/Literature/Lunn et al - 2000 - WinBUGS - A Bayesian modelling framework.pdf}
}

@article{deValpine2002,
  title = {Review of Methods for Fitting Time-Series Models with Process and Observation Error and Likelihood Calculations for Nonlinear, Non-{{Gaussian}} State-Space Models},
  volume = {70},
  abstract = {A key challenge for analyzing fisheries time-series data has been to incorporate sources of uncertainty such as process error, observation error, and model-structure uncertainty. Recent years have seen promising advances in methods for handling the first two together in a state-space framework, but likelihood calculations for state-space models require high-dimensional integrals, which make their use computationally challenging. The first section of this paper reviews model-fitting methods that use a state-space model structure, including errors-in-variables methods, Bayesian methods that do and do not use the state-space likelihood, and the possibility of classical likelihood analysis with nonlinear, non-Gaussian state-space models. It also discusses the relationship between true likelihood calculations and errors-in-variables likelihoods, as well as the role of Monte Carlo methods in implementing Bayesian and/or state-space model analyses. The second section introduces a numerical method for calculating state-space likelihoods without Monte Carlo methods and gives examples in a classical maximum-likelihood framework. The method is applicable when the dimension of the state space at each time step is low. Although recent advances in model-fitting and analysis methods are promising, inferences from noisy data and complex processes will continue to be variable and uncertain.},
  number = {2},
  journaltitle = {Bulletin of Marine Science},
  shortjournal = {Bull. Mar. Sci.},
  date = {2002},
  pages = {455-471},
  author = {de Valpine, Perry},
  options = {useprefix=true},
  file = {/home/jkbest/Zotero/storage/3CA9FLI6/de Valpine - 2002 - Review of Methods for Fitting Time-Series Models w.pdf;/home/jkbest/Zotero/storage/5FRCKMCP/display.html}
}

@article{Fox1970,
  langid = {english},
  title = {An Exponential Surplus-Yield Model for Optimizing Exploited Fish Populations},
  volume = {99},
  issn = {1548-8659},
  url = {https://afspubs.onlinelibrary.wiley.com/doi/abs/10.1577/1548-8659%281970%2999%3C80%3AAESMFO%3E2.0.CO%3B2},
  doi = {10.1577/1548-8659(1970)99<80:AESMFO>2.0.CO;2},
  abstract = {A surplus-yield model of fishery dynamics which assumes the Gompertz growth function is developed, resulting in an implied exponential relationship between catch per unit effort and fishing effort, and in an asymmetrical yield curve. A maximum sustainable yield, predicted by the exponential model, is obtained from a population size which is about 37\% of the environmentally limited maximum size. Three methods for estimating the parameters of the exponential model, adapted from those used for the linear model of Schaefer (1954, 1957), are presented. The exponential model is compared with the linear model using examples of the fisheries for the California sardine, Sardinops caerulea (Girard), and yellowfin tuna, Thunnus albacares (Bonnaterre) of the eastern tropical Pacific and western Atlantic Oceans. Management implications are discussed.},
  number = {1},
  journaltitle = {Transactions of the American Fisheries Society},
  shortjournal = {Trans. Am. Fish. Soc.},
  urldate = {2018-11-12},
  date = {1970-01-01},
  pages = {80-88},
  keywords = {Surplus production model},
  author = {Fox, William W.},
  file = {/home/jkbest/Documents/Literature/Fox - 1970 - An Exponential Surplus-Yield Model for Optimizing .pdf;/home/jkbest/Documents/Literature/Fox - 1970 - An Exponential Surplus-Yield Model for Optimizing Exploited Fish Populations.pdf;/home/jkbest/Zotero/storage/EE5IZIWK/1548-8659(1970)9980AESMFO2.0.html;/home/jkbest/Zotero/storage/KHM6ASQG/1548-8659(1970)9980AESMFO2.0.html}
}

@article{Ono2012,
  title = {Model Performance Analysis for {{Bayesian}} Biomass Dynamics Models Using Bias, Precision and Reliability Metrics},
  volume = {125-126},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783612001026},
  doi = {10.1016/j.fishres.2012.02.022},
  abstract = {Bayesian observation error (OEM), process error (PEM) and state-space (SSM) implementations of a Fox biomass dynamics model are compared using a simulation–estimation approach and by applying them to data for the octopus fishery off Mauritania. Estimation performance is evaluated in terms of bias, precision, and reliability measured by the extreme tail-area probability and the mean highest posterior density interval. The PEM generally performs poorest of the three methods in terms of the these performance metrics. In contrast, the OEM is precise, but under-represents uncertainty. The OEM is outperformed by the SSM in terms of its ability to provide posterior distributions which adequately capture parameter uncertainty. It is key to consider the above four metrics when comparing estimation performance in a Bayesian context. Finally, although model performance measures are useful, there is still a need to examine goodness of fit statistics in actual applications.},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2018-11-12},
  date = {2012-08-01},
  pages = {173-183},
  keywords = {Bayesian,Model performance,Octopus,OpenBUGS,State-space model},
  author = {Ono, Kotaro and Punt, André E. and Rivot, Etienne},
  file = {/home/jkbest/Documents/Literature/Ono et al - 2012 - Model performance analysis for Bayesian biomass dynamics models using bias,.pdf;/home/jkbest/Documents/Literature/Ono et al - 2012 - Model performance analysis for Bayesian biomass dynamics models using bias,2.pdf;/home/jkbest/Zotero/storage/CEFIJEHF/S0165783612001026.html;/home/jkbest/Zotero/storage/CS8E9BXN/S0165783612001026.html}
}

@article{Montenegro2016,
  title = {Bayesian State-Space Approach to Biomass Dynamic Models with Skewed and Heavy-Tailed Error Distributions},
  volume = {181},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783616300893},
  doi = {10.1016/j.fishres.2016.03.021},
  abstract = {We use the state-space approach to the logistic population growth model to update our knowledge of a population of marine shrimp off the Chilean coast. The unobserved state is the annual shrimp biomass, and the observation is the mean catch per unit effort. The observation equation is linear, and the state equation is nonlinear. The models include normal, student-t, skew-normal, and skew-t distributions for additive observation errors; and log-normal, log-t, log-skew-normal, and log-skew-t distributions for multiplicative observation errors. We use Bayesian approach to obtain inference, and the posterior distributions are approximated using Markov chain Monte Carlo methods. Deviance Information Criteria are lower in models considering log-skew-normal and log-skew-t observation errors. Furthermore, considering the posterior predictive distributions of the autocorrelations of the observation errors, these two models work best for the analyzed data set.},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2018-11-12},
  date = {2016-09-01},
  pages = {48-62},
  keywords = {State-space models,Surplus production model,Bayesian MCMC,Chilean shrimp,Logistic growth model,Nonlinear dynamic models},
  author = {Montenegro, Carlos and Branco, Márcia},
  file = {/home/jkbest/Documents/Literature/Montenegro Branco - 2016 - Bayesian state-space approach to biomass dynamic models with skewed and.pdf;/home/jkbest/Zotero/storage/7PSGY2SG/S0165783616300893.html}
}

@article{deValpine2002a,
  langid = {english},
  title = {Fitting Population Models Incorporating Process Noise and Observation Error},
  volume = {72},
  issn = {1557-7015},
  url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/0012-9615%282002%29072%5B0057%3AFPMIPN%5D2.0.CO%3B2},
  doi = {10.1890/0012-9615(2002)072[0057:FPMIPN]2.0.CO;2},
  abstract = {We evaluate a method for fitting models to time series of population abundances that incorporates both process noise and observation error in a likelihood framework. The method follows the probability logic of the Kalman filter, but whereas the Kalman filter applies to linear, Gaussian systems, we implement the full probability calculations numerically so that any nonlinear, non-Gaussian model can be used. We refer to the method as the “numerically integrated state-space (NISS) method” and compare it to two common methods used to analyze nonlinear time series in ecology: least squares with only process noise (LSPN) and least squares with only observation error (LSOE). We compare all three methods by fitting Beverton-Holt and Ricker models to many replicate model-generated time series of length 20 with several parameter choices. For the Ricker model we chose parameters for which the deterministic part of the model produces a stable equilibrium, a two-cycle, or a four-cycle. For each set of parameters we used three process-noise and observation-error scenarios: large standard deviation (0.2) for both, and large for one but small (0.05) for the other. The NISS method had lower estimator bias and variance than the other methods in nearly all cases. The only exceptions were for the Ricker model with stable-equilibrium parameters, in which case the LSPN and LSOE methods has lower bias when noise variances most closely met their assumptions. For the Beverton-Holt model, the NISS method was much less biased and more precise than the other methods. We also evaluated the utility of each method for model selection by fitting simulated data to both models and using information criteria for selection. The NISS and LSOE methods showed a strong bias toward selecting the Ricker over the Beverton-Holt, even when data were generated with the Beverton-Holt. It remains unclear whether the LSPN method is generally superior for model selection or has fortuitously better biases in this particular case. These results suggest that information criteria are best used with caution for nonlinear population models with short time series. Finally we evaluated the convergence of likelihood ratios to theoretical asymptotic distributions. Agreement with asymptotic distributions was very good for stable-point Ricker parameters, less accurate for two-cycle and four-cycle Ricker parameters, and least accurate for the Beverton-Holt model. The numerically integrated state-space method has a number of advantages over least squares methods and offers a useful tool for connecting models and data and ecology.},
  number = {1},
  journaltitle = {Ecological Monographs},
  shortjournal = {Ecol. Monogr.},
  urldate = {2018-11-24},
  date = {2002-02-01},
  pages = {57-76},
  keywords = {Ricker model,parameter estimation,time series,Beverton-Holt model,Kalman filter,least-squares cf. state-space models,model-fitting,observation error,population models,process noise},
  author = {de Valpine, Perry and Hastings, Alan},
  options = {useprefix=true},
  file = {/home/jkbest/Documents/Literature/de Valpine and Hastings - 2002 - Fitting Population Models Incorporating Process No.pdf;/home/jkbest/Zotero/storage/VAWZR2ST/0012-9615(2002)072[0057FPMIPN]2.0.html}
}

@article{Schnute1977,
  title = {Improved Estimates from the {{Schaefer}} Production Model: Theoretical Considerations},
  volume = {34},
  issn = {0015-296X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f77-094},
  doi = {10.1139/f77-094},
  shorttitle = {Improved {{Estimates}} from the {{Schaefer Production Model}}},
  abstract = {The Schaefer production model is converted to a form directly applicable to a data stream of annual fishing efforts and catches. The new version is also stochastic; that is, it allows for unpredictable influences on the fishery. A new method for estimating optimum effort and catch results from this analysis, as well as a way of measuring uncertainty in these estimates. Equations are given for predicting the next annual catch and assigning confidence limits to this prediction. Linear and nonlinear regressions are proposed for this analysis, and the relationship between them is rigorously demonstrated. The linear method leads to estimation formulas simple enough to be applied on a programmable pocket calculator. Key words: Schaefer model, production model, stochastic model, management, fishing effort, catch per unit effort, maximum sustainable yield, Le modèle de production de Schaefer est modifié de façon à être directement applicable à une série de données annuelles sur les efforts de pêche et sur les prises. La nouvelle version est également stochastique; c'est à dire qu'il y a place pour les facteurs imprévisibles influant sur les pêches. Il résulte de cette analyse une nouvelle méthode d'estimer les résultats optimaux relatifs aux efforts et aux captures ainsi qu'une façon de mesurer l'incertitude reliée à ces estimations. Des équations permettent de prévoir les captures de l'année suivante et d'en déterminer les limites de confiance. Des régressions linéaires et non linéaires sont proposées pour cette analyse, et la relation entre elles est rigoureusement démontrée. La méthode linéaire va de pair avec des formules d'estimation assez simples pour être résolues sur une calculette programmable.},
  number = {5},
  journaltitle = {Journal of the Fisheries Research Board of Canada},
  shortjournal = {J. Fish. Res. Bd. Can.},
  urldate = {2018-11-25},
  date = {1977-05-01},
  pages = {583-603},
  author = {Schnute, J.},
  file = {/home/jkbest/Documents/Literature/Schnute - 1977 - Improved Estimates from the Schaefer Production Model.pdf;/home/jkbest/Zotero/storage/A5CLQKMP/f77-094.html}
}

@article{Punt2003,
  title = {Extending Production Models to Include Process Error in the Population Dynamics},
  volume = {60},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f03-105},
  doi = {10.1139/f03-105},
  abstract = {Four methods for fitting production models, including three that account for the effects of error in the population dynamics equation (process error) and when indexing the population (observation error), are evaluated by means of Monte Carlo simulation. An estimator that represents the distributions of biomass explicitly and integrates over the unknown process errors numerically (the NISS estimator) performs best of the four estimators considered, never being the worst estimator and often being the best in terms of the medians of the absolute values of the relative errors. The total-error approach outperforms the observation-error estimator conventionally used to fit dynamic production models, and the performance of a Kalman filter based estimator is particularly poor. Although the NISS estimator is the best-performing estimator considered, its estimates of quantities of management interest are severely biased and highly imprecise for some of the scenarios considered., Une simulation de Monte Carlo permet d'évaluer quatre méthodes d'ajustement des modèles de production, dont trois qui prennent en compte les effets des erreurs dans la dynamique de population (erreurs de processus) et dans l'indexation de la population (erreurs d'observation). Des quatre estimateurs examinés, l'estimateur NISS, qui représente la distribution de la biomasse de façon explicite et qui fait numériquement l'intégration des erreurs de processus inconnues, fonctionne le mieux; il n'est jamais le pire et est très souvent le meilleur en ce qui concerne les médianes des valeurs absolues des erreurs relatives. La méthode de l'erreur totale fonctionne mieux que celle de l'erreur d'observation couramment utilisée dans l'ajustement de modèles dynamiques de production; la méthode qui utilise un filtre de Kalman fonctionne particulièrement mal. Bien que la méthode NISS soit la meilleure des méthodes étudiées, les estimations d'intérêt pour la gestion qu'elle produit sont particulièrement faussées et très...},
  number = {10},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-11-25},
  date = {2003-10-01},
  pages = {1217-1228},
  author = {Punt, Andre E},
  file = {/home/jkbest/Documents/Literature/Punt - 2003 - Extending production models to include process error in the population dynamics.pdf;/home/jkbest/Zotero/storage/RI5BME4T/f03-105.html}
}

@article{Ludwig1985,
  title = {Are Age-Structured Models Appropriate for Catch-Effort Data?},
  volume = {42},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f85-132},
  doi = {10.1139/f85-132},
  abstract = {Simulated data have been used to evaluate the performance of schemes for estimating optimum fishing effort using a simple stock-production model and R. B. Deriso's age-structured model Even when the data are generated using Deriso's model, the simpler production model generally gives as good or better estimates for the optimal effort. The only exception to this result is when data are provided with unrealistically large contrasts in effort and catch per unit effort over time. The implication of these findings is that simple production models should often be used in stock assessments based on catch/effort data, even when more realistic and structurally correct models are available to the analyst; the best choice depends on how much contrast has occurred in the historical effort and catch per unit effort data, rather than on prior knowledge about which model structure is biologically more realistic., Les auteurs ont utilisé des données simulées pour évaluer le rendement de plans d'estimation de l'effort de pêche optimum, à l'aide d'un simple modèle de production de stock et du modèle de R. B. Deriso structuré selon l'âge. En général, le simple modèle de production fournit des estimations aussi bonnes, voire meilleures de l'effort optimal même quand les données sont obtenues du modèle de Deriso, avec une seule exception : quand il y a beaucoup trop d'écart entre les données sur l'effort et les prises par unité d'effort en fonction du temps. Ces résultats indiquent que de simples modèles de production devraient souvent être utilisés pour les évaluations de stock basées sur les données de prises et d'effort, même quand des modèles plus réalistes et structurellement corrects sont disponibles; le meilleur choix dépend de l'écart qui s'est produit dans les données historiques sur l'effort et les prises par unité d'effort et non d'une connaissance antérieure du modèle le plus biologiquement réaliste.},
  number = {6},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-12-02},
  date = {1985-06-01},
  pages = {1066-1072},
  author = {Ludwig, Donald and Walters, Carl J.},
  file = {/home/jkbest/Documents/Literature/Ludwig Walters - 1985 - Are Age-Structured Models Appropriate for Catch-Effort Data.pdf;/home/jkbest/Zotero/storage/MLWL5QT9/f85-132.html}
}

@article{Millar2000a,
  title = {Bayesian State-Space Modeling of Age-Structured Data: Fitting a Model Is Just the Beginning},
  volume = {57},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f99-169},
  doi = {10.1139/f99-169},
  shorttitle = {Bayesian State-Space Modeling of Age-Structured Data},
  abstract = {Explicit modeling of process variability in the dynamics of fisheries is motivated by a desire to incorporate more realism into stock assessment models, and much recent research effort has been devoted to the computational features of fitting state-space models for this purpose. Here, we extend the Bayesian application of nonlinear state-space modeling to sequential population analysis of age-structured data using a model formulation that allows for unreported catches and incidental fishing mortality. It is shown that, once a familiarity with the general-purpose Bayesian software BUGS is acquired, implementing a state-space model is a relatively simple task. Indeed, this application requires just 18 lines of code in its entirety and does not require the programmer to know the formulae for any prior density functions or likelihoods. Consequently, we suggest that this methodology may permit the implementation phase of nonlinear state-space modeling to be relegated, thereby allowing more effort to be devoted..., La modélisation explicite de la variabilité des processus en dynamique des pêches découle de l'intention de rendre plus réalistes les modèles d'évaluation des stocks, et beaucoup de recherches ont été consacrées récemment aux caractéristiques computationnelles de l'ajustement des modèles état-espace. Dans le présent cas, nous étendons l'application bayésienne de la modélisation non-linéaire état-espace à l'analyse séquentielle de population de données structurées par âges en faisant appel à une formulation du modèle qui permet de tenir compte des captures non signalées et de la mortalité par pêche accidentelle. Il est montré que l'utilisation d'un modèle état-espace s'avère relativement simple une fois que l'on connaît assez bien le logiciel bayésien général BUGS. Une telle application n'exige que 18 lignes de codes au total et le programmeur n'a pas à connaître les formules des fonctions de densité ou de ressemblance antérieures. Nous croyons donc que cette méthode pourrait permettre de délaisser l'étape...},
  number = {1},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-12-02},
  date = {2000-01-01},
  pages = {43-50},
  author = {Millar, Russell B and Meyer, Renate},
  file = {/home/jkbest/Documents/Literature/Millar Meyer - 2000 - Bayesian state-space modeling of age-structured data.pdf;/home/jkbest/Zotero/storage/3UAXV2DW/f99-169.html}
}

@article{Abia2005,
  title = {Age-Structured Population Models and Their Numerical Solution},
  volume = {188},
  issn = {0304-3800},
  url = {http://www.sciencedirect.com/science/article/pii/S0304380005002450},
  doi = {10.1016/j.ecolmodel.2005.05.007},
  abstract = {This paper considers the state of the art of the numerical solution of age-structured population models. The different numerical approaches to this kind of problems and the stability and convergence results for them are reviewed. Both characteristic curves methods and finite difference methods are compared with regards to accuracy, efficiency and their qualitative behaviour depending on the compatibility conditions between initial and boundary data of the problems. The paper is the first of a series of two considering the numerical solution of general structured population models.},
  number = {1},
  journaltitle = {Ecological Modelling},
  shortjournal = {Ecol. Model.},
  series = {Special {{Issue}} on {{Theoretical Ecology}} and {{Mathematical Modelling}}: {{Problems}} and {{Methods}}},
  urldate = {2018-12-02},
  date = {2005-10-25},
  pages = {112-136},
  keywords = {Age-structured population models,Characteristic curves methods,Finite difference methods,Gurtin–MacCamy equation,Initial and boundary data compatibility conditions},
  author = {Abia, L. M. and Angulo, O. and López-Marcos, J. C.},
  file = {/home/jkbest/Documents/Literature/Abia et al - 2005 - Age-structured population models and their numerical solution.pdf;/home/jkbest/Zotero/storage/GYZVRNCS/S0304380005002450.html}
}

@article{Tahvonen2008,
  langid = {english},
  title = {Harvesting an Age-Structured Population as Biomass: Does It Work?},
  volume = {21},
  issn = {1939-7445},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1939-7445.2008.00022.x},
  doi = {10.1111/j.1939-7445.2008.00022.x},
  shorttitle = {Harvesting an {{Age}}-{{Structured Population}} as {{Biomass}}},
  abstract = {The economics of fisheries is based heavily on describing fish populations by the surplus production model. Both economists and ecologists have different opinions on whether this approach provides an adequate biological basis for economic analysis. This study takes an age-structured population model and shows how, under equilibrium conditions, it determines the surplus production model. The surplus production model is then used to solve an optimal feedback policy for a generic optimal harvesting problem. Next, it is assumed that the fishery manager applies this feedback policy even though the fish population actually evolves according to the age-structured model. This framework is applied to the widow rockfish, Atlantic menhaden, and Pacific halibut fisheries. Population age-structure contains information on future harvest possibilities. The surplus production model neglects this information and may lead to major deviations between the expected and actual outcomes especially under multiple steady states and nonlinearities.},
  number = {4},
  journaltitle = {Natural Resource Modeling},
  shortjournal = {Nat. Resour. Model.},
  urldate = {2018-12-02},
  date = {2008-12-01},
  pages = {525-550},
  keywords = {age-structured population models,fishery economics,Optimal harvesting,surplus production models},
  author = {Tahvonen, Olli},
  file = {/home/jkbest/Zotero/storage/4QRFHBU9/Tahvonen - 2008 - Harvesting an Age-Structured Population as Biomass.pdf;/home/jkbest/Zotero/storage/7Y6EREGB/j.1939-7445.2008.00022.html}
}

@article{Pella1969,
  title = {A Generalized Stock Production Model},
  volume = {13},
  url = {http://aquaticcommons.org/3536/1/Vol._13_no._3.pdf},
  abstract = {A generalization fo the Schaefer model is descibed which allows for skewness of the stock production curve relating production with population size. A fitting scheme is developed by which the stock production curve can be determined for an exploited fishery using only the catch and effort history of the fishery. Examplies are provided which demonstrate the suitability of the model for describing the dynamics of certain fish populations. In particular the catch and effort history for the yellowfin tuna fishery in the eastern Pacific Ocean is analyzed.},
  number = {3},
  journaltitle = {Inter-American Tropical Tuna Commission Bulletin},
  shortjournal = {Inter-Am. Trop. Tuna Comm. Bull.},
  urldate = {2018-12-02},
  date = {1969},
  pages = {421-458},
  keywords = {Surplus production model},
  author = {Pella, Jerome J. and Tomlinson, Patrick K.},
  file = {/home/jkbest/Zotero/storage/299NGKTT/Vol._13_no._3.pdf}
}

@article{Wood2010,
  langid = {english},
  title = {Statistical Inference for Noisy Nonlinear Ecological Dynamic Systems},
  volume = {466},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature09319},
  doi = {10.1038/nature09319},
  abstract = {Chaotic ecological dynamic systems defy conventional statistical analysis. Systems with near-chaotic dynamics are little better. Such systems are almost invariably driven by endogenous dynamic processes plus demographic and environmental process noise, and are only observable with error. Their sensitivity to history means that minute changes in the driving noise realization, or the system parameters, will cause drastic changes in the system trajectory1. This sensitivity is inherited and amplified by the joint probability density of the observable data and the process noise, rendering it useless as the basis for obtaining measures of statistical fit. Because the joint density is the basis for the fit measures used by all conventional statistical methods2, this is a major theoretical shortcoming. The inability to make well-founded statistical inferences about biological dynamic models in the chaotic and near-chaotic regimes, other than on an ad hoc basis, leaves dynamic theory without the methods of quantitative validation that are essential tools in the rest of biological science. Here I show that this impasse can be resolved in a simple and general manner, using a method that requires only the ability to simulate the observed data on a system from the dynamic model about which inferences are required. The raw data series are reduced to phase-insensitive summary statistics, quantifying local dynamic structure and the distribution of observations. Simulation is used to obtain the mean and the covariance matrix of the statistics, given model parameters, allowing the construction of a ‘synthetic likelihood’ that assesses model fit. This likelihood can be explored using a straightforward Markov chain Monte Carlo sampler, but one further post-processing step returns pure likelihood-based inference. I apply the method to establish the dynamic nature of the fluctuations in Nicholson’s classic blowfly experiments3,4,5.},
  number = {7310},
  journaltitle = {Nature},
  shortjournal = {Nature},
  urldate = {2018-12-06},
  date = {2010-08},
  pages = {1102-1104},
  author = {Wood, Simon N.},
  file = {/home/jkbest/Documents/Literature/Wood - 2010 - Statistical inference for noisy nonlinear ecological dynamic systems.pdf;/home/jkbest/Zotero/storage/JCPGWDV5/nature09319.html}
}

@article{Geyer1992,
  title = {Practical {{Markov}} Chain {{Monte Carlo}}},
  volume = {7},
  issn = {0883-4237},
  url = {https://www.jstor.org/stable/2246094},
  abstract = {Markov chain Monte Carlo using the Metropolis-Hastings algorithm is a general method for the simulation of stochastic processes having probability densities known up to a constant of proportionality. Despite recent advances in its theory, the practice has remained controversial. This article makes the case for basing all inference on one long run of the Markov chain and estimating the Monte Carlo error by standard nonparametric methods well-known in the time-series and operations research literature. In passing it touches on the Kipnis-Varadhan central limit theorem for reversible Markov chains, on some new variance estimators, on judging the relative efficiency of competing Monte Carlo schemes, on methods for constructing more rapidly mixing Markov chains and on diagnostics for Markov chain Monte Carlo.},
  number = {4},
  journaltitle = {Statistical Science},
  shortjournal = {Stat. Sci.},
  urldate = {2018-12-06},
  date = {1992},
  pages = {473-483},
  author = {Geyer, Charles J.},
  file = {/home/jkbest/Zotero/storage/GHDMGPM4/Geyer - 1992 - Practical Markov Chain Monte Carlo.pdf}
}

@article{Polacheck1993,
  title = {Fitting Surplus Production Models: Comparing Methods and Measuring Uncertainty},
  volume = {50},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f93-284},
  doi = {10.1139/f93-284},
  shorttitle = {Fitting {{Surplus Production Models}}},
  abstract = {Three approaches are commonly used to fit surplus production models to observed data: effort-averaging methods; process-error estimators; and observation-error estimators. We compare these approaches using real and simulated data sets, and conclude that they yield substantially different interpretations of productivity. Effort-averaging methods assume the stock is in equilibrium relative to the recent effort; this assumption is rarely satisfied and usually leads to overestimation of potential yield and optimum effort. Effort-averaging methods will almost always produce what appears to be "reasonable" estimates of maximum sustainable yield and optimum effort, and the r2 statistic used to evaluate the goodness of fit can provide an unrealistic illusion of confidence about the parameter estimates obtained. Process-error estimators produce much less reliable estimates than observation-error estimators. The observation-error estimator provides the lowest estimates of maximum sustainable yield and optimum effor..., On emploie communément trois méthodes pour ajuster les modèles de production excédentaire aux résultats observés; il y a les méthodes de la moyenne d'effort, les estimateurs des erreurs de traitement ainsi que les estimateurs des erreurs d'observation. Nous comparons ces trois démarches au moyen d'ensembles de données réelles et simulées, et nous parvenons à la conclusion que ces méthodes conduisent à des interprétations largement différentes de la productivité. Les méthodes fondées sur les moyennes d'effort supposent que le stock est en équilibre relativement à l'effort récent; c'est rarement le cas, mais cela conduit ordinairement à une surestimation du rendement potentiel et de l'effort optimal. Ces méthodes produiront presque toujours ce qui semble être des estimations « raisonnables » du rendement soutenable maximal et de l'effort optimal, et la valeur statistique r2 qui sert à évaluer la validité de l'ajustement peut donner l'illusion non fondée de confiance dans les estimations des paramètres qui s...},
  number = {12},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-12-07},
  date = {1993-12-01},
  pages = {2597-2607},
  author = {Polacheck, Tom and Hilborn, Ray and Punt, Andre E.},
  file = {/home/jkbest/Documents/Literature/Polacheck et al - 1993 - Fitting Surplus Production Models.pdf;/home/jkbest/Zotero/storage/NI8ASIAE/f93-284.html}
}

@article{Colvin2012,
  title = {Semidiscrete Biomass Dynamic Modeling: An~Improved Approach for Assessing Fish Stock Responses to Pulsed Harvest Events},
  volume = {69},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/full/10.1139/f2012-084},
  doi = {10.1139/f2012-084},
  shorttitle = {Semidiscrete Biomass Dynamic Modeling},
  abstract = {Continuous harvest over an annual period is a common assumption of continuous biomass dynamics models (CBDMs); however, fish are frequently harvested in a discrete manner. We developed semidiscrete biomass dynamics models (SDBDMs) that allow discrete harvest events and evaluated differences between CBDMs and SDBDMs using an equilibrium yield analysis with varying levels of fishing mortality~(F). Equilibrium fishery yields for CBDMs and SDBDMS were similar at low fishing mortalities and diverged as F approached and exceeded maximum sustained yield (FMSY). Discrete harvest resulted in lower equilibrium yields at high levels of F relative to continuous harvest. The effect of applying harvest continuously when it was in fact discrete was evaluated by fitting CBDMs and SDBDMs to time series data generated from a hypothetical fish stock undergoing discrete harvest and evaluating parameter estimates bias. Violating the assumption of continuous harvest resulted in biased parameter estimates for CBDM while SDBDM p..., Si les modèles continus de dynamique de la biomasse (CBDM) partent souvent du principe que la capture est continue au cours d’une période annuelle, les prises de poissons s’effectuent fréquemment de manière discrète. Nous avons mis au point des modèles semidiscrets de dynamique de la biomasse (SDBDM) qui permettent l’intégration d’évènements de prise discrets et avons évalué les différences entre les CBDM et les SDBDM à la lumière d’une analyse du rendement équilibré à différents taux de mortalité par pêche (F). À de faibles F, les CBDM et les SDBDM ont donné des rendements équilibrés semblables, l’écart entre ces derniers augmentant à mesure que F s’approche puis dépasse le rendement maximum durable (FMSY). L’intégration de prises discrètes s’est traduite par des rendements équilibrés plus faibles à des F élevés que ceux obtenus pour des prises continues. L’incidence de l’utilisation de prises continues dans des cas où les prises sont en fait discrètes a été évaluée en ajustant les CBDM et les SDBDM aux ...},
  number = {10},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2018-12-07},
  date = {2012-09-28},
  pages = {1710-1721},
  author = {Colvin, Michael E. and Pierce, Clay L. and Stewart, Timothy W.},
  file = {/home/jkbest/Documents/Literature/Colvin et al - 2012 - Semidiscrete biomass dynamic modeling.pdf;/home/jkbest/Zotero/storage/T8B83MDE/f2012-084.html}
}

@article{Kalman1960,
  title = {A New Approach to Linear Filtering and Prediction Problems},
  volume = {82},
  issn = {0098-2202},
  url = {http://dx.doi.org/10.1115/1.3662552},
  doi = {10.1115/1.3662552},
  abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
  number = {1},
  journaltitle = {Journal of Basic Engineering},
  shortjournal = {J. Basic Eng.},
  urldate = {2018-12-14},
  date = {1960-03-01},
  pages = {35-45},
  author = {Kalman, R. E.},
  file = {/home/jkbest/Zotero/storage/TLVVHKH6/A New Approach to Linear Filtering and Prediction .pdf}
}

@incollection{Neal2011,
  langid = {english},
  location = {{Boca Raton}},
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  isbn = {978-1-4200-7942-5},
  url = {http://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=384516},
  abstract = {Markov chain Monte Carlo (MCMC) originated with the classic paper of Metropolis et al. (1953), where it was used to simulate the distribution of states for a system of idealized molecules. Not long after, another approach to molecular simulation was introduced (Alder and Wainwright, 1959), in which the motion of the molecules was deterministic, following Newton’s laws of motion, which have an elegant formalization as Hamiltonian dynamics. For finding the properties of bulk materials, these approaches are asymptotically equivalent, since even in a deterministic simulation, each local region of the material experiences effectively random influences from distant regions. Despite the large overlap in their application areas, the MCMC and molecular dynamics approaches have continued to coexist in the following decades (see Frenkel and Smit, 1996).},
  booktitle = {Handbook of {{Markov}} Chain {{Monte Carlo}}},
  series = {Handbooks of {{Modern Statistical Methods}}},
  publisher = {{Taylor \& Francis}},
  urldate = {2018-11-05},
  date = {2011},
  pages = {113-162},
  author = {Neal, Radford M.},
  editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiao-Li},
  note = {OCLC: 753969788}
}

@article{Gilks1994,
  eprinttype = {jstor},
  eprint = {2348941},
  title = {A Language and Program for Complex {{Bayesian}} Modelling},
  volume = {43},
  issn = {0039-0526},
  doi = {10.2307/2348941},
  abstract = {Gibbs sampling has enormous potential for analysing complex data sets. However, routine use of Gibbs sampling has been hampered by the lack of general purpose software for its implementation. Until now all applications have involved writing one-off computer code in low or intermediate level languages such as C or Fortran. We describe some general purpose software that we are currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling). The BUGS system comprises three components: first, a natural language for specifying complex models; second, an 'expert system' for deciding appropriate methods for obtaining samples required by the Gibbs sampler: third, a sampling module containing numerical routines to perform the sampling. \$S\$ objects are used for data input and output. BUGS is written in Modula-2 and runs under both DOS and UNIX.},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  shortjournal = {J. Royal Stat. Soc. D},
  date = {1994},
  pages = {169-177},
  author = {Gilks, W. R. and Thomas, A. and Spiegelhalter, D. J.},
  file = {/home/jkbest/Zotero/storage/5HWW4AU6/Gilks et al. - 1994 - A Language and Program for Complex Bayesian Modell.pdf}
}

@article{Kitagawa1996,
  eprinttype = {jstor},
  eprint = {1390750},
  title = {Monte {{Carlo}} Filter and Smoother for Non-{{Gaussian}} Nonlinear State Space Models},
  volume = {5},
  issn = {1061-8600},
  doi = {10.2307/1390750},
  abstract = {[A new algorithm for the prediction, filtering, and smoothing of non-Gaussian nonlinear state space models is shown. The algorithm is based on a Monte Carlo method in which successive prediction, filtering (and subsequently smoothing), conditional probability density functions are approximated by many of their realizations. The particular contribution of this algorithm is that it can be applied to a broad class of nonlinear non-Gaussian higher dimensional state space models on the provision that the dimensions of the system noise and the observation noise are relatively low. Several numerical examples are shown.]},
  number = {1},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {J. Comput. Graph. Stat.},
  date = {1996},
  pages = {1-25},
  author = {Kitagawa, Genshiro},
  file = {/home/jkbest/Zotero/storage/YWZY9PIZ/Monte Carlo Filter and Smoother for Non-Gaussian N.pdf}
}

@article{Carlin1992,
  eprinttype = {jstor},
  eprint = {2290282},
  title = {A {{Monte Carlo}} Approach to Nonnormal and Nonlinear State-Space Modeling},
  volume = {87},
  issn = {0162-1459},
  doi = {10.2307/2290282},
  abstract = {[A solution to multivariate state-space modeling, forecasting, and smoothing is discussed. We allow for the possibilities of nonnormal errors and nonlinear functionals in the state equation, the observational equation, or both. An adaptive Monte Carlo integration technique known as the Gibbs sampler is proposed as a mechanism for implementing a conceptually and computationally simple solution in such a framework. The methodology is a general strategy for obtaining marginal posterior densities of coefficients in the model or of any of the unknown elements of the state space. Missing data problems (including the k-step ahead prediction problem) also are easily incorporated into this framework. We illustrate the broad applicability of our approach with two examples: a problem involving nonnormal error distributions in a linear model setting and a one-step ahead prediction problem in a situation where both the state and observational equations are nonlinear and involve unknown parameters.]},
  number = {418},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J. Am. Stat. Assoc.},
  date = {1992},
  pages = {493-500},
  author = {Carlin, Bradley P. and Polson, Nicholas G. and Stoffer, David S.},
  file = {/home/jkbest/Documents/Literature/Carlin et al - 1992 - A Monte Carlo Approach to Nonnormal and Nonlinear State-Space Modeling.pdf}
}

@article{Plummer2003,
  langid = {english},
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  abstract = {JAGS is a program for Bayesian Graphical modelling which aims for compatibility with Classic BUGS. The program could eventually be developed as an R package. This article explains the motivations for this program, brieﬂy describes the architecture and then discusses some ideas for a vectorized form of the BUGS language.},
  journaltitle = {Working Papers},
  date = {2003},
  pages = {8},
  author = {Plummer, Martyn},
  file = {/home/jkbest/Zotero/storage/W7AKLKYG/Plummer - 2003 - JAGS A program for analysis of Bayesian graphical.pdf}
}

@article{Hoffman2014,
  langid = {english},
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: Adaptively Setting Path Lengths in {{Hamiltonian Monte Carlo}}},
  volume = {15},
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by ﬁrst-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC’s performance is highly sensitive to two user-speciﬁed parameters: a step size and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as eﬃciently as (and sometimes more eﬃciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter on the ﬂy based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require eﬃcient “turnkey” samplers.},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  date = {2014-04},
  pages = {1593-1623},
  author = {Hoffman, Matthew D and Gelman, Andrew},
  file = {/home/jkbest/Zotero/storage/3SMKRPZP/Hoﬀman and Gelman - The No-U-Turn Sampler Adaptively Setting Path Len.pdf}
}

@article{Vats2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.09384},
  primaryClass = {stat},
  title = {Revisiting the {{Gelman}}-{{Rubin Diagnostic}}},
  url = {http://arxiv.org/abs/1812.09384},
  abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods of variance estimation for Monte Carlo averages. We show that this class of estimators find immediate use in the Gelman-Rubin statistic, a connection not established in the literature before. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to increased stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled cutoff criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
  urldate = {2018-12-30},
  date = {2018-12-21},
  keywords = {Statistics - Methodology,Statistics - Computation},
  author = {Vats, Dootika and Knudson, Christina},
  file = {/home/jkbest/Documents/Literature/Vats Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;/home/jkbest/Zotero/storage/4AHCTLTC/1812.html}
}

@software{RCoreTeam2018,
  location = {{Vienna, Austria}},
  title = {R: A Language and Environment for Statistical Computing},
  url = {https://www.R-project.org/},
  version = {v3.5.2},
  organization = {{R Foundation for Statistical Computing}},
  date = {2018},
  author = {{R Core Team}}
}

@article{Gruss2019,
  title = {Evaluation of the Impacts of Different Treatments of Spatio-Temporal Variation in Catch-per-Unit-Effort Standardization Models},
  volume = {213},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783619300086},
  doi = {10.1016/j.fishres.2019.01.008},
  abstract = {Many stock assessments heavily rely on indices of relative abundance derived from fisheries-dependent catch-per-unit-effort (CPUE) data. Therefore, it is critical to evaluate different CPUE standardization methods under varying scenarios of data generating processes. Here, we evaluated nine CPUE standardization methods offering contrasting treatments of spatio-temporal variation, ranging from the basic generalized linear model (GLM) method not integrating a year-area interaction term to a sophisticated method using the spatio-temporal modeling platform VAST. We compared the performance of these methods against simulated data constructed to mimic the processes generating fisheries-dependent information for Atlantic blue marlin (Makaira nigricans), a common bycatch population in pelagic longline fisheries. Data were generated using a longline data simulator for different population trajectories (increasing, decreasing, and static). These data were further subsampled to mimic an observer program where trips rather than sets form the sampling frame, with or without a bias towards trips with low catch rates, which might occur if the presence of an observer alters fishing behavior to avoid bycatch. The spatio-temporal modeling platform VAST achieved the best performance in simulation, namely generally had one of the lowest biases, one of the lowest mean absolute errors (MAEs), and 50\% confidence interval coverage closest to 50\%. Generalized additive models accounting for spatial autocorrelation at a broad spatial scale (one of the lowest MAEs and one of the lowest biases) and, to a lesser extent, non-spatial delta-lognormal GLMs including a year-area interaction as a random effect (one of the lowest MAEs and one of the best confidence interval coverages) also performed adequately. The VAST method provided the most comprehensive and consistent treatment of spatio-temporal variation, in contrast with methods that simply weight predictions by large spatial areas, where it is critical, but difficult, to get the a priori spatial stratification correct before weighting. Next, we applied the CPUE standardization methods to real data collected by the National Marine Fisheries Service Pelagic Observer Program. The indices of relative abundance predicted from real observer data were relatively similar across CPUE standardization methods for the period 1998–2017 and suggested that the blue marlin population of the Atlantic declined over the period 1998–2004 and was relatively stable afterwards. As spatio-temporal variation related to environmental changes or depletion becomes increasingly necessary to consider, greater use of spatio-temporal models for standardizing fisheries-dependent CPUE data will likely be warranted.},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2019-01-22},
  date = {2019-05-01},
  pages = {75-93},
  keywords = {Catch-per-unit-effort (CPUE),Indices of relative abundance,Simulation-testing,Spatio-temporal models,Standardization methods},
  author = {Grüss, Arnaud and Walter, John F. and Babcock, Elizabeth A. and Forrestal, Francesca C. and Thorson, James T. and Lauretta, Matthew V. and Schirripa, Michael J.},
  file = {/home/jkbest/Documents/Literature/Grüss et al - 2019 - Evaluation of the impacts of different treatments of spatio-temporal variation.pdf;/home/jkbest/Zotero/storage/8G6Y6JXP/S0165783619300086.html}
}

@article{Millar2002,
  title = {Reference Priors for {{Bayesian}} Fisheries Models},
  volume = {59},
  issn = {0706-652X},
  url = {http://www.nrcresearchpress.com/doi/abs/10.1139/f02-108},
  doi = {10.1139/f02-108},
  abstract = {Bayesian models require the specification of prior distributions for all unknown parameters, and this formal utilization of prior knowledge (if any) can be used to great advantage in some fisheries. However, regardless of whether prior knowledge about model parameters is available, specification of prior distributions is seldom unequivocal. This work addresses the problem of specifying default priors for several common fisheries models. To maintain consistency of terminology with the statistical literature, such priors are herein called reference priors to recognize that they can be interpreted as providing a sensible reference point against which the implications of alternative priors can be compared. Here, the Jeffreys' prior is demonstrated for the Ricker and BevertonHolt stockrecruit curves, von Bertalanffy growth curve, Schaefer surplus production model, and sequential population analysis. The Jeffreys' priors for relevant derived parameters are demonstrated, including the steepness parameter of th..., Les modèles bayésiens exigent que l'on spécifie des distributions a priori pour tous les paramètres inconnus et cette utilisation formelle de données a priori (lorsqu'elles existent) est d'une grande utilité dans l'étude de certaines pêches commerciales. Cependant, qu'il y ait ou non des informations a priori sur les paramètres du modèle, l'attribution des distributions a priori est rarement sans équivoque. On trouvera ici un examen du problème d'assigner des distributions a priori dans plusieurs modèles courants utilisés en halieutique. Pour maintenir une uniformité de terminologie avec la littérature statistique, ces distributions à priori sont appelées distributions a priori de référence pour indiquer qu'elles peuvent procurer un point de référence crédible auquel les caractéristiques d'autres distributions a priori de rechange peuvent être comparées. La distribution a priori de Jeffreys est déterminée pour les courbes de stockrecrutement de Ricker et de BevertonHolt, la courbe de croissance de von B...},
  number = {9},
  journaltitle = {Canadian Journal of Fisheries and Aquatic Sciences},
  shortjournal = {Can. J. Fish. Aquat. Sci.},
  urldate = {2019-02-02},
  date = {2002-09-01},
  pages = {1492-1502},
  author = {Millar, Russell B},
  file = {/home/jkbest/Zotero/storage/CYLWT5BL/Millar - 2002 - Reference priors for Bayesian fisheries models.pdf;/home/jkbest/Zotero/storage/XUKGNY3N/f02-108.html}
}

@article{Thorson2014a,
  langid = {english},
  title = {A {{Bayesian}} Approach to Identifying and Compensating for Model Misspecification in Population Models},
  volume = {95},
  issn = {1939-9170},
  url = {http://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/13-0187.1},
  doi = {10.1890/13-0187.1},
  abstract = {State-space estimation methods are increasingly used in ecology to estimate productivity and abundance of natural populations while accounting for variability in both population dynamics and measurement processes. However, functional forms for population dynamics and density dependence often will not match the true biological process, and this may degrade the performance of state-space methods. We therefore developed a Bayesian semiparametric state-space model, which uses a Gaussian process (GP) to approximate the population growth function. This offers two benefits for population modeling. First, it allows data to update a specified “prior” on the population growth function, while reverting to this prior when data are uninformative. Second, it allows variability in population dynamics to be decomposed into random errors around the population growth function (“process error”) and errors due to the mismatch between the specified prior and estimated growth function (“model error”). We used simulation modeling to illustrate the utility of GP methods in state-space population dynamics models. Results confirmed that the GP model performs similarly to a conventional state-space model when either (1) the prior matches the true process or (2) data are relatively uninformative. However, GP methods improve estimates of the population growth function when the function is misspecified. Results also demonstrated that the estimated magnitude of “model error” can be used to distinguish cases of model misspecification. We conclude with a discussion of the prospects for GP methods in other state-space models, including age and length-structured, meta-analytic, and individual-movement models.},
  number = {2},
  journaltitle = {Ecology},
  shortjournal = {Ecol.},
  urldate = {2019-02-02},
  date = {2014},
  pages = {329-341},
  keywords = {Gaussian process,state-space models,Bayesian models,model misspecification,population dynamics model,semiparametric models},
  author = {Thorson, James T. and Ono, Kotaro and Munch, Stephan B.},
  file = {/home/jkbest/Documents/Literature/Thorson et al - 2014 - A Bayesian approach to identifying and compensating for model misspecification.pdf;/home/jkbest/Zotero/storage/2JEIXM9R/13-0187.html}
}

@article{Zhou2010,
  langid = {english},
  title = {Modified Hierarchical {{Bayesian}} Biomass Dynamics Models for Assessment of Short-Lived Invertebrates: A Comparison for Tropical Tiger Prawns},
  volume = {60},
  issn = {1448-6059},
  url = {http://www.publish.csiro.au/mf/mf09022},
  doi = {10.1071/MF09022},
  shorttitle = {Modified Hierarchical {{Bayesian}} Biomass Dynamics Models for Assessment of Short-Lived Invertebrates},
  abstract = {Conventional biomass dynamics models express next year’s biomass as this year’s biomass plus surplus production less catch. These models are typically applied to species with several age-classes but it is unclear how well they perform for short-lived species with low survival and high recruitment variation. Two alternative versions of the standard biomass dynamics model (Standard) were constructed for short-lived species by ignoring the ‘old biomass’ term (Annual), and assuming that the biomass at the start of the next year depends on density-dependent processes that are a function of that biomass (Stock-recruit). These models were fitted to catch and effort data for the grooved tiger prawn Penaeus semisulcatus using a hierarchical Bayesian technique. The results from the biomass dynamics models were compared with those from more complicated weekly delay-difference models. The analyses show that: the Standard model is flexible for short-lived species; the Stock-recruit model provides the most parsimonious fit; simple biomass dynamics models can provide virtually identical results to data-demanding models; and spatial variability in key population dynamics parameters exists for P. semisulacatus. The method outlined in this paper provides a means to conduct quantitative population assessments for data-limited short-lived species.},
  number = {12},
  journaltitle = {Marine and Freshwater Research},
  shortjournal = {Mar. Freshw. Res.},
  urldate = {2019-02-05},
  date = {2010-01-23},
  pages = {1298-1308},
  author = {Zhou, Shijie and Punt, André E. and Deng, Roy and Dichmont, Catherine M. and Ye, Yimin and Bishop, Janet},
  file = {/home/jkbest/Documents/Literature/Zhou et al - 2010 - Modified hierarchical Bayesian biomass dynamics models for assessment of.pdf;/home/jkbest/Documents/Literature/Zhou et al - 2010 - Modified hierarchical Bayesian biomass dynamics models for assessment of2.pdf;/home/jkbest/Zotero/storage/6M3FYKI9/mf09022.html;/home/jkbest/Zotero/storage/XQ9G2IT3/mf09022.html}
}

@article{Jiao2011,
  langid = {english},
  title = {Poor-Data and Data-Poor Species Stock Assessment Using a {{Bayesian}} Hierarchical Approach},
  volume = {21},
  issn = {1939-5582},
  url = {http://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/10-0526.1},
  doi = {10.1890/10-0526.1},
  abstract = {Appropriate inference for stocks or species with low-quality data (poor data) or limited data (data poor) is extremely important. Hierarchical Bayesian methods are especially applicable to small-area, small-sample-size estimation problems because they allow poor-data species to borrow strength from species with good-quality data. We used a hammerhead shark complex as an example to investigate the advantages of using hierarchical Bayesian models in assessing the status of poor-data and data-poor exploited species. The hammerhead shark complex (Sphyrna spp.) along the Atlantic and Gulf of Mexico coasts of the United States is composed of three species: the scalloped hammerhead (S. lewini), the great hammerhead (S. mokarran), and the smooth hammerhead (S. zygaena) sharks. The scalloped hammerhead comprises 70–80\% of the catch and has catch and relative abundance data of good quality, whereas great and smooth hammerheads have relative abundance indices that are both limited and of low quality presumably because of low stock density and limited sampling. Four hierarchical Bayesian state-space surplus production models were developed to simulate variability in population growth rates, carrying capacity, and catchability of the species. The results from the hierarchical Bayesian models were considerably more robust than those of the nonhierarchical models. The hierarchical Bayesian approach represents an intermediate strategy between traditional models that assume different population parameters for each species and those that assume all species share identical parameters. Use of the hierarchical Bayesian approach is suggested for future hammerhead shark stock assessments and for modeling fish complexes with species-specific data, because the poor-data species can borrow strength from the species with good data, making the estimation more stable and robust.},
  number = {7},
  journaltitle = {Ecological Applications},
  shortjournal = {Ecol. Appl.},
  urldate = {2019-02-05},
  date = {2011},
  pages = {2691-2708},
  keywords = {population dynamics,Bayesian hierarchical model,data-poor assessment,fish complex,hammerhead shark,small sample size},
  author = {Jiao, Yan and Cortés, Enric and Andrews, Kate and Guo, Feng},
  file = {/home/jkbest/Documents/Literature/Jiao et al - 2011 - Poor-data and data-poor species stock assessment using a Bayesian hierarchical.pdf;/home/jkbest/Documents/Literature/Jiao et al - 2011 - Poor-data and data-poor species stock assessment using a Bayesian hierarchical2.pdf;/home/jkbest/Zotero/storage/NVN6WFVV/10-0526.html;/home/jkbest/Zotero/storage/UER3U9JQ/10-0526.html}
}

@article{Pedersen2011,
  title = {Estimation Methods for Nonlinear State-Space Models in Ecology},
  volume = {222},
  issn = {0304-3800},
  url = {http://www.sciencedirect.com/science/article/pii/S0304380011000299},
  doi = {10.1016/j.ecolmodel.2011.01.007},
  abstract = {The use of nonlinear state-space models for analyzing ecological systems is increasing. A wide range of estimation methods for such models are available to ecologists, however it is not always clear, which is the appropriate method to choose. To this end, three approaches to estimation in the theta logistic model for population dynamics were benchmarked by Wang (2007). Similarly, we examine and compare the estimation performance of three alternative methods using simulated data. The first approach is to partition the state-space into a finite number of states and formulate the problem as a hidden Markov model (HMM). The second method uses the mixed effects modeling and fast numerical integration framework of the AD Model Builder (ADMB) open-source software. The third alternative is to use the popular Bayesian framework of BUGS. The study showed that state and parameter estimation performance for all three methods was largely identical, however with BUGS providing overall wider credible intervals for parameters than HMM and ADMB confidence intervals.},
  number = {8},
  journaltitle = {Ecological Modelling},
  shortjournal = {Ecol. Model.},
  urldate = {2019-02-05},
  date = {2011-04-24},
  pages = {1394-1400},
  keywords = {AD Model Builder,WinBUGS,Hidden Markov model,Mixed model,Monte Carlo,Theta logistic population model},
  author = {Pedersen, M. W. and Berg, C. W. and Thygesen, U. H. and Nielsen, A. and Madsen, H.},
  file = {/home/jkbest/Documents/Literature/Pedersen et al - 2011 - Estimation methods for nonlinear state-space models in ecology.pdf;/home/jkbest/Zotero/storage/WPVTYKKM/S0304380011000299.html}
}

@article{Dichmont2016,
  title = {A Review of Stock Assessment Packages in the {{United States}}},
  volume = {183},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783616302132},
  doi = {10.1016/j.fishres.2016.07.001},
  abstract = {Stock assessments provide scientific advice in support of fisheries decision making. Ideally, assessments involve fitting population dynamics models to fishery and monitoring data to provide estimates of time-trajectories of biomass and fishing mortality in absolute terms and relative to biological reference points such as BMSY and FMSY, along with measures of uncertainty. Some stock assessments are conducted using software developed for a specific stock or group of stocks. However, increasingly, stock assessments are being conducted using packages developed for application to several taxa and across multiple regions. We review the range of packages used to conduct assessments of fish and invertebrate stocks in the United States because these assessments tend to have common goals, and need to provide similar outputs for decision making. Sixteen packages are considered, five based on surplus production models, one based on a delay-difference model, and the remainder based on age-structured models. Most of the packages are freely available for use by analysts in the US and around the world, have been evaluated using simulations, and can form the basis for forecasts. The packages differ in their ease of use and the types of data inputs they can use. This paper highlights the benefits of stock assessment packages in terms of allowing analysts to explore many assessment configurations and facilitating the peer-review of assessments. It also highlights the disadvantages associated with the use of packages for conducting assessments. Packages with the most options and greatest flexibility are the most difficult to use, and see the greatest development of auxiliary tools to facilitate their use.},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2019-02-07},
  date = {2016-11-01},
  pages = {447-460},
  keywords = {Stock assessment,Fishing mortality,Population dynamics,Reference points},
  author = {Dichmont, Catherine M. and Deng, Roy A. and Punt, Andre E. and Brodziak, Jon and Chang, Yi-Jay and Cope, Jason M. and Ianelli, James N. and Legault, Christopher M. and Methot, Richard D. and Porch, Clay E. and Prager, Michael H. and Shertzer, Kyle W.},
  file = {/home/jkbest/Documents/Literature/Dichmont et al - 2016 - A review of stock assessment packages in the United States.pdf;/home/jkbest/Zotero/storage/6FEV8SXQ/S0165783616302132.html}
}

@article{ICCAT2016,
  langid = {english},
  title = {Report of the 2015 {{ICCAT}} Blue Shark Stock Assessment Session},
  volume = {72},
  abstract = {The meeting was held at the Oceanário de Lisboa, Portugal, 27-31 July. The objective of this
meeting was to assess the status of the stocks (North and South) of Atlantic blue shark. The last
assessment was conducted in 2008 and targeting of longline fisheries has developed in recent
years.},
  number = {4},
  journaltitle = {Collected Volumes of Scientific Papers},
  date = {2016},
  pages = {866-1019},
  author = {ICCAT},
  file = {/home/jkbest/Zotero/storage/DB9DQ799/de Lisboa - REPORT OF THE 2015 ICCAT BLUE SHARK STOCK ASSESSME.pdf}
}

@article{ISC2017,
  title = {Stock Assessment and Future Projections of Blue Shark in the {{North Pacific Ocean}} through 2015},
  journaltitle = {International Scientific Committee for tuna and tuna-like species in the North Pacific Ocean},
  date = {2017-07-12},
  pages = {96},
  author = {ISC},
  file = {/home/jkbest/Zotero/storage/XWP58Z6J/ISC - 2017 - Annex 13 - Stock assessment and future projections_of_blue_shark.pdf}
}

@article{ICCAT2017,
  langid = {english},
  title = {Report of the 2017 {{ICCAT}} Shortfin Mako Stock Assessment Meeting},
  volume = {74},
  abstract = {The meeting was held in Madrid Spain, 12-16 June. The objective of this meeting was to assess
the status of the stocks (North and South) of Atlantic shortfin mako shark. The last assessment
was conducted in 2012. The populations were assessed using several models, from different types
of surplus production models to fully integrated age-structured models. For the first time,
projections of stock status were conducted for this species and management advice was provided
based on Kobe strategy matrices. The assessment represented a significant step forward in the
understanding of shortfin mako populations in the Atlantic Ocean.},
  number = {4},
  journaltitle = {Collected Volumes of Scientific Papers},
  shortjournal = {Collect. Vol. Sci. Pap.},
  date = {2017},
  pages = {1465-1561},
  author = {ICCAT},
  file = {/home/jkbest/Zotero/storage/HJ66FUK3/ICCAT - 2017 - Report of the 2017 ICCAT shortfin mako assessment meeting.pdf}
}

@article{ICCAT2017a,
  langid = {english},
  title = {Report of the 2017 {{ICCAT Atlantic}} Swordfish Stock Assessment Session},
  volume = {74},
  abstract = {The meeting was held in Madrid Spain, 12-16 June. The objective of this meeting was to assess
the status of the stocks (North and South) of Atlantic shortfin mako shark. The last assessment
was conducted in 2012. The populations were assessed using several models, from different types
of surplus production models to fully integrated age-structured models. For the first time,
projections of stock status were conducted for this species and management advice was provided
based on Kobe strategy matrices. The assessment represented a significant step forward in the
understanding of shortfin mako populations in the Atlantic Ocean.},
  number = {3},
  journaltitle = {Collected Volumes of Scientific Papers},
  shortjournal = {Collect. Vol. Sci. Pap.},
  date = {2017},
  pages = {841-967},
  author = {ICCAT},
  file = {/home/jkbest/Zotero/storage/JGE5NDUN/ICCAT - 2017 - Report of the 2017 ICCAT Atlantic swordfish stock assessment session.pdf}
}

@article{ICCAT2017b,
  langid = {english},
  title = {Report of the 2017 {{ICCAT}} Albacore Species Group Intersessional Meeting (Including Assessment of {{Mediterranean}} Albacore)},
  volume = {74},
  abstract = {An ICCAT Albacore species group intersessional meeting was held in Madrid, Spain, 5-9 June
2017. The Group produced a stock assessment for the Mediterranean stock (last assessed in
2011), based on data poor methods. Likewise, substantial progress was made by the Group on
the development of the MSE framework, namely testing Limit Reference Points and HCRs for
north Atlantic albacore, and improved CPUE series for both northern and southern Albacore.},
  number = {2},
  journaltitle = {Collected Volumes of Scientific Papers},
  shortjournal = {Collect. Vol. Sci. Pap.},
  date = {2017},
  pages = {508-583},
  author = {ICCAT},
  file = {/home/jkbest/Zotero/storage/MV4ZCZL6/ICCAT - 2017 - Report of the 2017 ICCAT albacore species group intersessional meeting.pdf}
}

@article{ICCAT2014,
  langid = {english},
  title = {Report of the 2013 {{Atlantic}} Swordfish Stock Assessment Session},
  volume = {70},
  abstract = {Atlantic Swordfish Stock Assessment Session. The meeting was held in Olhȧo, Portugal, 2-10
September 2013. The objective of the meeting was to carry out stock assessments of the North
and South Atlantic swordfish stocks.},
  number = {2},
  journaltitle = {Collected Volumes of Scientific Papers},
  shortjournal = {Collect. Vol. Sci. Pap.},
  date = {2014},
  pages = {1484-1678},
  author = {ICCAT},
  file = {/home/jkbest/Zotero/storage/J6UN3JK3/ICCAT - 2014 - Report of the 2013 Atlantic swordfish stock assessment session.pdf}
}

@article{Carvalho2014,
  title = {Incorporating Specific Change Points in Catchability in Fisheries Stock Assessment Models: {{An}} Alternative Approach Applied to the Blue Shark ({{Prionace}} Glauca) Stock in the South {{Atlantic Ocean}}},
  volume = {154},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783614000381},
  doi = {10.1016/j.fishres.2014.01.022},
  shorttitle = {Incorporating Specific Change Points in Catchability in Fisheries Stock Assessment Models},
  abstract = {Fishermen frequently switch their target fish species without documenting changes in which species they are targeting and the fishing practices used, generating misleading catchability information about the fish caught. To date, changes in target species have been incorporated in stock assessments at two different levels in analyses. First, these changes are taken into account during the parameterization of generalized linear models used to compute the CPUE index standardization. Second, changes in target species are directly incorporated as a time-varying catchability parameter during the fitting of the dynamic model used for the assessment. Here, we present an alternative method for this incorporation by specifying a single change point in the stationary distribution of the catchability coefficient in a Bayesian state-space production model. Two models were fitted to the time series of the south Atlantic blue shark (Prionace glauca) stock. In one of the models, only one catchability coefficient was estimated. In the other model, a change point was included, and two catchability coefficients were estimated, one before the change point, and the other after. Despite the latter model introducing an extra parameter, it produced a significantly better fit than the modeling approach without the change point. Although including a single change point in the catchability coefficient had no significant impact on the status of south Atlantic blue shark (which is still above BMSY), it provided a robust way of accounting for changes in catchability as a result of fishermen changing target species.},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2019-02-07},
  date = {2014-06-01},
  pages = {135-146},
  keywords = {Stock assessment,CPUE standardization,Blue shark,Catchability,South Atlantic},
  author = {Carvalho, Felipe and Ahrens, Robert and Murie, Debra and Ponciano, José M. and Aires-da-Silva, Alexandre and Maunder, Mark N. and Hazin, Fábio},
  file = {/home/jkbest/Documents/Literature/Carvalho et al - 2014 - Incorporating specific change points in catchability in fisheries stock.pdf;/home/jkbest/Zotero/storage/LVN4ZY58/S0165783614000381.html}
}

@article{Chang2015,
  title = {Model Selection and Multi-Model Inference for {{Bayesian}} Surplus Production Models: {{A}} Case Study for {{Pacific}} Blue and Striped Marlin},
  volume = {166},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783614002781},
  doi = {10.1016/j.fishres.2014.08.023},
  shorttitle = {Model Selection and Multi-Model Inference for {{Bayesian}} Surplus Production Models},
  abstract = {Stock assessment typically involves developing a set of alternative models, fitting each to the available data, and then selecting the one that gives the most accurate estimates of management quantities of interest. In this context, it is important to consider model selection uncertainty because ignoring it can lead to unreliable estimates and overconfident inferences. For this study, four Bayesian surplus production models with symmetric or asymmetric production functions and either a constant or hierarchical time-varying intrinsic growth rate (r) were developed using data for Pacific blue marlin (Makaira nigricans) and Western and Central North Pacific striped marlin (Kajikia audax). The uncertainty resulting from model selection was evaluated using Monte Carlo simulation techniques to examine the consistency of model estimates within (self-tests) and among (cross-tests) the alternative models. Specifically, these tests evaluated the performance of the deviance information criterion (DIC) and Bayesian model averaging (BMA). The results of the simulation tests suggested that mis-specification of time-varying r can lead to large estimation errors for biomass and management quantities and that DIC may not reliably identify the true data-generating model. Although BMA did not provide more accurate point estimates than just selecting the data-generating model, it did provide a more accurate characterization of uncertainty in model results. Our study shows the value of using simulations to evaluate model performance and to account for model selection uncertainty.},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  series = {Proceedings of the 5th {{International Billfish Symposium}}},
  urldate = {2019-02-07},
  date = {2015-06-01},
  pages = {129-139},
  keywords = {Multi-model inference,Deviance information criterion,Bayesian hierarchical surplus production model,Billfish,Model selection uncertainty},
  author = {Chang, Yi-Jay and Brodziak, Jon and O’Malley, Joseph and Lee, Hui-Hua and DiNardo, Gerard and Sun, Chi-Lu},
  file = {/home/jkbest/Documents/Literature/Chang et al - 2015 - Model selection and multi-model inference for Bayesian surplus production models.pdf;/home/jkbest/Zotero/storage/T8YRYB9Z/Chang et al. - 2015 - Model selection and multi-model inference for Baye.pdf;/home/jkbest/Zotero/storage/IX9DTUEA/S0165783614002781.html}
}

@article{McAllister2014,
  langid = {english},
  title = {A Generalized {{Bayesian}} Surplus Production Stock Assessment Software ({{BSP2}})},
  volume = {70},
  abstract = {A generalized Bayesian surplus production stock assessment software (BSP2) is presented as an update to ICCAT’s current BSP software. BSP2 differs from BSP in a few different respects. Most importantly, BSP2 provides a state-space implementation of the deterministic Bayesian generalized surplus production model found in BSP. BSP2 models both process error in the dynamics equations to account for the effects of, e.g., interannual variation in recruitment, and error in predicted observations. BPS2 provides outputs to enable the computation of Bayes factors (BFs). BFs show Bayes posterior weights for different models and can be particularly important when structurally different models suggest different interpretations of stock status. A generalized production function implementation (i.e., the Fletcher model) is incorporated in which the ratio of the most productive stock size to carrying capacity can be set at hypothesized values other than the Schaefer model value of 0.5. The software can accommodate a variety of different priors for key parameters including carrying capacity (K), the maximum rate of population increase (r), and the ratio of stock biomass in the initial year to carrying capacity (Binit/K).},
  number = {4},
  journaltitle = {Collected Volumes of Scientific Papers, ICCAT},
  date = {2014},
  pages = {1725-1757},
  author = {McAllister, Murdoch K},
  file = {/home/jkbest/Zotero/storage/CR98V4BW/McAllister - 2014 - A generalized Bayesian surplus production stock as.pdf}
}

@article{Prager2002,
  title = {Comparison of Logistic and Generalized Surplus-Production Models Applied to Swordfish, {{Xiphias}} Gladius, in the North {{Atlantic Ocean}}},
  volume = {58},
  issn = {0165-7836},
  url = {http://www.sciencedirect.com/science/article/pii/S0165783601003587},
  doi = {10.1016/S0165-7836(01)00358-7},
  abstract = {Recent assessments of swordfish, Xiphias gladius, in the north Atlantic Ocean by the International Commission for the Conservation of Atlantic Tunas (ICCAT) have included fitting a nonequilibrium logistic (Schaefer) surplus-production model. The logistic model offers simplicity, but concern has been expressed that its fixed model shape may bias estimates of quantities of management interest. Here, I compare results from the logistic estimator used by ICCAT to those from an otherwise equivalent generalized (Pella–Tomlinson) production-model estimator. Following initial estimation with nonlinear least-squares, a resistant fitting method was used to identify statistical outliers, and both models were refit with outliers removed. The estimate of model shape from the generalized model was then close to the logistic, and estimates of stock status from the two estimators were similar. A simulation study conditioned on the trimmed generalized fit suggests that any systematic estimation error caused by assuming logistic shape for this stock is small. Moreover, the generalized estimator was sensitive to outlying observations and thus less precise than the logistic estimator, and it exhibited larger median proportional unsigned error. Sensitivity to outliers and lack of precision in an estimator make it more likely to provide misleading estimates in a given analysis; therefore, if the generalized production model with estimated shape parameter is used in stock assessment, it should be applied with skepticism and in conjunction with the more robust logistic form. Unless a good external estimate of model shape is available, the logistic model appears more suitable for routine assessment use on stocks similar to swordfish.},
  number = {1},
  journaltitle = {Fisheries Research},
  shortjournal = {Fish. Res.},
  urldate = {2019-02-07},
  date = {2002-10-01},
  pages = {41-57},
  keywords = {Stock assessment,Swordfish,Accuracy,North Atlantic Ocean,Outliers,Precision,Robust methods,Surplus production},
  author = {Prager, Michael H.},
  file = {/home/jkbest/Documents/Literature/Prager - 2002 - Comparison of logistic and generalized surplus-production models applied to.pdf;/home/jkbest/Zotero/storage/XU4PL9DI/S0165783601003587.html}
}

@article{Lunn2009,
  langid = {english},
  title = {The {{BUGS}} Project: {{Evolution}}, Critique and Future Directions},
  volume = {28},
  issn = {1097-0258},
  url = {https://www.onlinelibrary.wiley.com/doi/abs/10.1002/sim.3680},
  doi = {10.1002/sim.3680},
  shorttitle = {The {{BUGS}} Project},
  abstract = {BUGS is a software package for Bayesian inference using Gibbs sampling. The software has been instrumental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the BUGS project and some future perspectives. Copyright © 2009 John Wiley \& Sons, Ltd.},
  number = {25},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat. Med.},
  urldate = {2019-02-08},
  date = {2009},
  pages = {3049-3067},
  keywords = {WinBUGS,BUGS,OpenBUGS,Bayesian modelling,graphical models},
  author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
  file = {/home/jkbest/Documents/Literature/Lunn et al - 2009 - The BUGS project.pdf;/home/jkbest/Zotero/storage/A7IUCH3U/sim.html}
}

@article{Zerbini2011,
  langid = {english},
  title = {A {{Bayesian}} Assessment of the Conservation Status of Humpback Whales ({{Megaptera}} Novaeangliae) in the Western {{South Atlantic Ocean}}},
  volume = {Special issue},
  abstract = {The population of humpback whales (Megaptera novaeangliae) wintering off the eastern coast of South America is referred to by the International Whaling Commission as ‘Breeding Stock A’ (BSA). This population was heavily exploited in 20th century modern commercial whaling operations. After more than 30 years of protection, its present status remains unknown. A deterministic sex and age-aggregated population dynamics model was used to estimate the pre-exploitation population size (K), the maximum net recruitment rate (rmax), the maximum depletion level (Nmin/K), and other quantities of interest of BSA. Input data included modern whaling catch series, absolute estimates of abundance, observed growth rates and indices of relative abundance. A Bayesian statistical method was used to calculate probability distributions for the model parameters. Prior distributions were set on rmax – an uninformative (Uniform [0, 0.106]) and an informative (Normal [0.067, 0.042]) – and on the population size in 2005 – N2005 (Uniform [500, 22,000]). A total of 10,000 samples were used to compute the joint posterior distribution of the model parameters using the Sampling-Importance-Resampling algorithm. Sensitivity of model outputs to the priors on rmax, a genetic constraint, data inclusion and catch allocation scenarios was investigated. Medians of the posterior probability distributions of quantities of interest for the base case scenario were: rmax = 0.069 (95\% probability intervals [PI] = 0.013–0.104), K = 24,558 (95\% PI = 22,791–31,118), Nmin/K = 2\% (PI = 0.31\%–12.5\%), N2006/ K = 27.4\% (PI = 18.3\%–39.5\%), N2020/K = 61.8\% (PI = 23.8\%–88.6\%), and N2040/K = 97.3\% (PI = 31.6\%–99.9\%). Despite apparent recovery in the past three decades, the western South Atlantic humpback whale population is still low relative to its pre-exploitation size and requires continued conservation efforts.},
  number = {3},
  journaltitle = {Journal of Cetacean Research and Management},
  shortjournal = {J. Cetacean Res. Manage.},
  date = {2011},
  pages = {131-144},
  author = {Zerbini, Alexandre N and Ward, Eric J and Kinas, Paul G and Engel, Márcia H and Andriolo, Artur},
  file = {/home/jkbest/Zotero/storage/IAUDV6KA/Zerbini et al. - 2011 - A Bayesian assessment of the conservation status o.pdf}
}


